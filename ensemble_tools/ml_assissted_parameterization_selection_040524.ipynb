{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a7bd05",
   "metadata": {},
   "outputs": [],
   "source": [
    "machine = \"derecho\" #\"laptop_Z30\" or \"derecho\"\n",
    "\n",
    "if machine == \"laptop_Z30\":\n",
    "    path_to_base_param_files_root = '/home/adamhb/gdrive/postdoc/parameters'\n",
    "    path_to_ensemble_param_files_root = '/home/adamhb/gdrive/postdoc/parameters/ensemble_1280_041724'\n",
    "    path_to_ca_fates = '/home/adamhb/gdrive/postdoc/california-fates'\n",
    "    path_to_esm_tools = '/home/adamhb/gdrive/FATES/Earth-System-Model-Tools'\n",
    "    \n",
    "if machine == \"derecho\":\n",
    "    path_to_base_param_files_root = '/glade/u/home/adamhb/ahb_params/fates_api_25'\n",
    "    path_to_ensemble_param_files_root = '/glade/u/home/adamhb/ahb_params/fates_api_25/ensembles/'\n",
    "    path_to_ca_fates = '/glade/u/home/adamhb/california-fates/'\n",
    "    path_to_esm_tools = '/glade/u/home/adamhb/Earth-System-Model-Tools/'\n",
    "\n",
    "# from sklearn.model_selection import cross_val_score\n",
    "# from sklearn.model_selection import cross_val_predict\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "# import tensorflow\n",
    "# import keras\n",
    "# from keras import models\n",
    "# from keras import layers\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import Dense\n",
    "# from keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import io\n",
    "import numpy as np\n",
    "import os\n",
    "from scipy.stats import qmc\n",
    "pd.set_option('display.max_rows', 500) \n",
    "import shutil\n",
    "import netCDF4 as nc4\n",
    "import sys\n",
    "sys.path.append(\"/glade/u/home/adamhb/california-fates/ensemble_tools/\")\n",
    "sys.path.append(path_to_esm_tools)\n",
    "import esm_tools\n",
    "import modp as mp\n",
    "from scipy.io import netcdf as nc\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f470988e",
   "metadata": {},
   "source": [
    "## Specify paths and load emulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06787ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load parameter ranges\n",
    "param_ranges_full = pd.read_csv(os.path.join(path_to_ca_fates,'parameter_ranges/param_range_archive/param_ranges_040524.csv'))\n",
    "#param_ranges_full = pd.read_csv(os.path.join(path_to_ca_fates,'parameter_ranges/param_range_archive/param_ranges_100223.csv'))\n",
    "\n",
    "# The parameter file used for parameter values that don't vary in the ensemble \n",
    "base_param_file_name = 'ca_ahb_5pfts_041624.nc'\n",
    "\n",
    "# New param file name prefix for the parameter files that do vary\n",
    "file_name_prefix = 'ca_5pfts_100523'\n",
    "\n",
    "# Instances per case\n",
    "inst_per_case = 128\n",
    "\n",
    "# Cases\n",
    "n_cases = 10\n",
    "\n",
    "target_ensemble_size = inst_per_case * n_cases\n",
    "\n",
    "# Prefix for subdirectories where param files for the ensemble are kept\n",
    "param_files_subdir_prefix = f'afterBugFix_{target_ensemble_size}_041923'\n",
    "\n",
    "# How large should each lhs sample be in the generator\n",
    "batch_size = target_ensemble_size\n",
    "seed = 17\n",
    "\n",
    "#path_DNN_NFailed_PFTs = os.path.join(path_to_ca_fates,'ml_emulators/saved_models/DNN_NFailed_110323_v1.h5')\n",
    "#DNN_NFailed_PFTs = keras.models.load_model(path_DNN_NFailed_PFTs)\n",
    "#DNN_NFailed_PFTs = tf.keras.models.load_model(path_DNN_NFailed_PFTs)\n",
    "#path_to_ensemble_data = os.path.join(path_to_ca_fates,'ml_emulators/ML_training_data_110323.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ebc4da",
   "metadata": {},
   "source": [
    "## Check that saved model is working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9192d442",
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_df = pd.read_csv(path_to_ensemble_data)\n",
    "# raw_df.info()\n",
    "# print(\"\\n\")\n",
    "\n",
    "# # Drop first column\n",
    "# raw_df.drop(columns=raw_df.columns[0], axis=1, inplace=True)\n",
    "# print(list(raw_df.columns))\n",
    "\n",
    "# df = raw_df.copy()\n",
    "# df.fillna(0, inplace=True)\n",
    "\n",
    "# from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "# split = StratifiedShuffleSplit(n_splits=1, test_size = 0.1, random_state = 42)\n",
    "\n",
    "# # Split the data into training and test sets, maintaining an equal proportion of veg and non-veg labels in each.\n",
    "# for train_index, test_index in split.split(df, df['FailedPFTs']):\n",
    "#     train_set = df.iloc[train_index]\n",
    "#     test_set = df.iloc[test_index]\n",
    "\n",
    "# X_cols = train_set.columns[train_set.columns.str.contains('fates')]\n",
    "# train_X = train_set[X_cols]\n",
    "# test_X = test_set[X_cols]\n",
    "\n",
    "# #Create vars to be predicted\n",
    "# train_y_failedPFTs = train_set[\"FailedPFTs\"].copy()\n",
    "\n",
    "# test_y_failedPFTs = test_set[\"FailedPFTs\"].copy()\n",
    "\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# # This is for if I need to define a custom data transformation function\n",
    "# from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# # Data transformation pipeline\n",
    "# transformation_pipeline = Pipeline([\n",
    "#         ('std_scaler', StandardScaler()), # Scale the data by substracting mean and dividing by sigma\n",
    "#     ])\n",
    "\n",
    "# # Apply transformation pipeline to training data\n",
    "# X = transformation_pipeline.fit_transform(train_X)\n",
    "\n",
    "# # Apply to test data\n",
    "# X_test = transformation_pipeline.fit_transform(test_X)\n",
    "\n",
    "\n",
    "# from sklearn.metrics import confusion_matrix, classification_report\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "\n",
    "# loss, accuracy = DNN_NFailed_PFTs.evaluate(X_test, test_y_failedPFTs)\n",
    "# print(\"Test Loss:\", loss)\n",
    "# print(\"Test Accuracy:\", accuracy)\n",
    "\n",
    "# probabilities = DNN_NFailed_PFTs.predict(X_test)\n",
    "# y_pred = np.argmax(probabilities, axis=1)\n",
    "# print(y_pred)\n",
    "\n",
    "# target_names = [str(i) + \"Failed\" for i in range(5)]\n",
    "\n",
    "# # Compute the confusion matrix\n",
    "# cm = confusion_matrix(test_y_failedPFTs, y_pred)\n",
    "\n",
    "# # Display the confusion matrix using Seaborn\n",
    "# plt.figure(figsize=(6, 5))\n",
    "# sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=target_names, yticklabels=target_names)\n",
    "# plt.ylabel('Actual')\n",
    "# plt.xlabel('Predicted')\n",
    "# plt.title('Confusion Matrix')\n",
    "# plt.show()\n",
    "\n",
    "# # Optionally, display the classification report for more detailed metrics\n",
    "# print(classification_report(test_y_failedPFTs, y_pred, target_names= target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0776702",
   "metadata": {},
   "source": [
    "## Load parameter ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d383648",
   "metadata": {},
   "outputs": [],
   "source": [
    "pft_names = ['pine','cedar','fir','shrub','oak']\n",
    "param_ranges_full = param_ranges_full[['pft_ineq','param', 'value_min', 'value_max', 'pft', 'organ']]\n",
    "\n",
    "# Define params that we are going to sample from the LHS\n",
    "param_ranges = param_ranges_full.loc[param_ranges_full['pft_ineq'] == 'FALSE']\n",
    "\n",
    "# Define params that we are going to alter using pft-inequalities\n",
    "param_ranges_inequalities = param_ranges_full.loc[param_ranges_full['pft_ineq'] != 'FALSE']\n",
    "param_ranges_inequalities = param_ranges_inequalities.dropna(how=\"all\")\n",
    "\n",
    "# Make sure min and max are floats\n",
    "convert_dict = {'value_min': float,\n",
    "                'value_max': float}\n",
    "\n",
    "param_ranges = param_ranges.astype(convert_dict)\n",
    "\n",
    "# number of parameters\n",
    "n_params = len(param_ranges)\n",
    "print(\"Number of params:\",n_params)\n",
    "\n",
    "# number of PFTs - '0' is global so subtract one\n",
    "n_pfts = max(len(pd.unique(param_ranges['pft'])) - 1, 1)\n",
    "print(\"Number of pfts:\", n_pfts)\n",
    "\n",
    "param_names = list(param_ranges.param)\n",
    "print(\"Param names:\",param_names)\n",
    "pfts = list(param_ranges.pft)\n",
    "organs = list(param_ranges.organ)\n",
    "param_ranges.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdee5487",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c98cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def contains_angle_brackets(s):\n",
    "    return '<' in s or '>' in s\n",
    "\n",
    "def get_param_value_from_ineq(ref_pft_value,min_val,max_val,pft_ineq):\n",
    "    '''returns the parameter value for the pft that depends on a reference pft value'''\n",
    "    \n",
    "    if contains_angle_brackets(pft_ineq):\n",
    "        if \">\" in pft_ineq:\n",
    "            min_val = ref_pft_value\n",
    "            new_value = np.random.uniform(float(min_val),float(max_val))\n",
    "        if \"<\" in pft_ineq:\n",
    "            max_val = ref_pft_value\n",
    "            new_value = np.random.uniform(float(min_val),float(max_val))\n",
    "    else:\n",
    "        new_value = ref_pft_value\n",
    "    \n",
    "    return new_value\n",
    "\n",
    "def find_duplicates(lst):\n",
    "    count = {}\n",
    "    duplicates = []\n",
    "\n",
    "    for item in lst:\n",
    "        if item in count:\n",
    "            count[item] += 1\n",
    "        else:\n",
    "            count[item] = 1\n",
    "\n",
    "    for key, value in count.items():\n",
    "        if value > 1:\n",
    "            duplicates.append(key)\n",
    "\n",
    "    return duplicates\n",
    "\n",
    "def remove_duplicates(lst):\n",
    "    no_duplicates = []\n",
    "    [no_duplicates.append(x) for x in lst if x not in no_duplicates]\n",
    "    return no_duplicates\n",
    "\n",
    "def add_pft_nums(input_string,numbers):\n",
    "    return [input_string + \"_{}\".format(i) for i in numbers]\n",
    "\n",
    "def add_param_names_for_writing_to_netcdf():\n",
    "    \n",
    "    '''It needs to have all the variable names and indices required to write a netcdf file with the parameter values\n",
    "    The variable names and indices required to write the netcdf file are contained in the column headers, but there are some exceptions:\n",
    "\n",
    "    * fates_leaf_slamax_{1:5} equals fates_leaf_sla25top{1:5}\n",
    "    * fates_allom_d2ca_coefficient_min_{1:5} equals fates_allom_d2ca_coefficient_max_{1:5}\n",
    "    * fates_frag_maxdecomp_{2,3,5} are calcuated as specific fractions of fates_frag_maxdecomp_1\n",
    "    * fates_mort_hf_sm_threshold_{1:5} are the same as fates_mort_hf_sm_threshold_0\n",
    "    * fates_recruit_inter_patch_disp_frac_{1:5} are same as fates_recruit_inter_patch_disp_frac_0\n",
    "    '''\n",
    "     \n",
    "    # These are added so that when we write to netcdf files we can just use the column headers as a guide for\n",
    "    # param name and index\n",
    "    \n",
    "    # add fates_leaf_slamax_{1:5} \n",
    "    slamax = add_pft_nums(\"fates_leaf_slamax\",[1,2,3,4,5])\n",
    "    \n",
    "    # add fates_allom_d2ca_coefficient_min_{1:5}\n",
    "    allom_d2ca = add_pft_nums(\"fates_allom_d2ca_coefficient_min\",[1,2,3,4,5])\n",
    "    \n",
    "    # add fates_frag_maxdecomp_{2,3,5}\n",
    "    maxdecomp = add_pft_nums(\"fates_frag_maxdecomp\",[2,3,5])\n",
    "    \n",
    "    # add fates_mort_hf_sm_threshold_{1:5}\n",
    "    hf = add_pft_nums(\"fates_mort_hf_sm_threshold\",[1,2,3,4,5])\n",
    "    \n",
    "    # add fates_recruit_inter_patch_disp_frac_{1:5}\n",
    "    disp = add_pft_nums(\"fates_recruit_inter_patch_disp_frac\",[1,2,3,4,5])\n",
    "    \n",
    "    return slamax + allom_d2ca + maxdecomp + hf + disp\n",
    "\n",
    "def generate_lhs_df(batch_size, seed, param_ranges):\n",
    "\n",
    "    sampler = qmc.LatinHypercube(d=n_params, seed=seed)\n",
    "    sample = sampler.random(n=batch_size)\n",
    "\n",
    "    # scale to parameter ranges\n",
    "    l_bounds = param_ranges['value_min']\n",
    "    u_bounds = param_ranges['value_max']\n",
    "\n",
    "    scaled_sample = qmc.scale(sample, l_bounds, u_bounds)\n",
    "\n",
    "    print(\"ensemble dimensions:\",scaled_sample.shape)\n",
    "\n",
    "    # Create a dataframe of the LHS ensemble\n",
    "    col_names = [v + \"_\" + str(p) for v,p in zip(param_names,pfts)]\n",
    "    lhs_df = pd.DataFrame(data=scaled_sample,columns=col_names)\n",
    "    \n",
    "    return lhs_df\n",
    "\n",
    "\n",
    "def add_inequality_cols(lhs_df,param_ranges_inequalities):\n",
    "\n",
    "    # Create column names of the inequalities parameters\n",
    "    col_names_inequalities = [v + \"_\" + str(p) for v,p in zip(param_ranges_inequalities[\"param\"],param_ranges_inequalities['pft'])]\n",
    "\n",
    "    # Add columns names for special cases\n",
    "    col_names_inequalities = col_names_inequalities + add_param_names_for_writing_to_netcdf()\n",
    "\n",
    "    col_names_inequalities = remove_duplicates(col_names_inequalities)\n",
    "    \n",
    "    def remove_element(lst, element_to_remove):\n",
    "        return [item for item in lst if item != element_to_remove]\n",
    " \n",
    "    col_names_inequalities = remove_element(col_names_inequalities, \"fates_frag_maxdecomp_0\")\n",
    "    \n",
    "    inequalities_df = pd.DataFrame(np.zeros((lhs_df.shape[0],len(col_names_inequalities))),columns=col_names_inequalities)\n",
    "\n",
    "    # Concatenate the dfs\n",
    "    df = pd.concat([lhs_df,inequalities_df], axis = 1)\n",
    "\n",
    "    return df\n",
    "\n",
    "def get_ref_pft_num(ineq):  \n",
    "    for p in pft_names:\n",
    "        if p in ineq:\n",
    "            return pft_names.index(p) + 1\n",
    "        \n",
    "def generate_parameter_values_for_inequalities(param_ranges_inequalities,df):\n",
    "    \n",
    "    print(\"Adding pft inequalities...\")\n",
    "    \n",
    "    for ensemble_member_row in df.index:\n",
    "    \n",
    "        for i in param_ranges_inequalities.index:\n",
    "\n",
    "            # input data from the inequalities df\n",
    "            d = param_ranges_inequalities.loc[i]\n",
    "\n",
    "            # Make sla max the same as slatop\n",
    "            if d['param'] == \"fates_leaf_slamax\":\n",
    "                for i in range(1,6):\n",
    "                    pft_to_assign_value_to = i\n",
    "                    reference_col = \"fates_leaf_slatop_\" + str(pft_to_assign_value_to)\n",
    "                    target_col = \"fates_leaf_slamax_\" + str(pft_to_assign_value_to)\n",
    "                    reference_val = df.loc[ensemble_member_row,reference_col]\n",
    "                    target_val = reference_val\n",
    "                    df.at[ensemble_member_row,target_col] = target_val\n",
    "                    if i == 1:\n",
    "                        df.at[ensemble_member_row,\"fates_leaf_slamax_0\"] = target_val\n",
    "\n",
    "            # Make d2camin the same as d2ca max\n",
    "            if d['param'] == \"fates_allom_d2ca_coefficient_min\":\n",
    "                for i in range(1,6):\n",
    "                    pft_to_assign_value_to = i\n",
    "                    reference_col = \"fates_allom_d2ca_coefficient_max_\" + str(pft_to_assign_value_to)\n",
    "                    target_col = \"fates_allom_d2ca_coefficient_min_\" + str(pft_to_assign_value_to)\n",
    "                    reference_val = df.loc[ensemble_member_row,reference_col]\n",
    "                    target_val = reference_val\n",
    "                    df.at[ensemble_member_row,target_col] = target_val\n",
    "                    if i == 1:\n",
    "                        df.at[ensemble_member_row,\"fates_allom_d2ca_coefficient_min_0\"] = target_val\n",
    "\n",
    "            # Calculate fates_frag_maxdecomp\n",
    "            if d['param'] == \"fates_frag_maxdecomp\":\n",
    "                col_name_suffix = d['organ']\n",
    "                reference_col = \"fates_frag_maxdecomp_0\"\n",
    "                target_col = \"fates_frag_maxdecomp_\" + str(int(col_name_suffix))\n",
    "                \n",
    "\n",
    "                reference_val = df.loc[ensemble_member_row,reference_col]\n",
    "\n",
    "                target_val = reference_val * d['value_max']\n",
    "\n",
    "                df.at[ensemble_member_row,target_col] = target_val\n",
    "\n",
    "            # Set fates_mort_hf_sm_threshold the same for all pfts\n",
    "            for i in range(1,6):\n",
    "                pft_to_assign_value_to = i\n",
    "                reference_col = \"fates_mort_hf_sm_threshold_0\"\n",
    "                target_col = \"fates_mort_hf_sm_threshold_\" + str(int(pft_to_assign_value_to))\n",
    "                \n",
    "                reference_val = df.loc[ensemble_member_row,reference_col]\n",
    "\n",
    "                target_val = reference_val\n",
    "                \n",
    "                df.at[ensemble_member_row,target_col] = target_val\n",
    "\n",
    "            # Set fates_mort_hf_sm_threshold the same for all pfts\n",
    "            for i in range(1,6):\n",
    "                pft_to_assign_value_to = i\n",
    "                reference_col = \"fates_recruit_inter_patch_disp_frac_0\"\n",
    "                target_col = \"fates_recruit_inter_patch_disp_frac_\" + str(int(pft_to_assign_value_to))\n",
    "                reference_val = df.loc[ensemble_member_row,reference_col]\n",
    "                target_val = reference_val\n",
    "                df.at[ensemble_member_row,target_col] = target_val\n",
    "            \n",
    "            # Set the inequalities\n",
    "            if get_ref_pft_num(d['pft_ineq']) is not None:\n",
    "                pft_to_assign_value_to = d[\"pft\"]\n",
    "                reference_pft_num = get_ref_pft_num(d['pft_ineq'])\n",
    "                reference_col = d['param'] + \"_\" + str(int(reference_pft_num))\n",
    "                reference_val = df.loc[ensemble_member_row,reference_col]\n",
    "                target_col = d['param'] + \"_\" + str(int(pft_to_assign_value_to))\n",
    "                target_val = get_param_value_from_ineq(reference_val,min_val = d['value_min'],\n",
    "                                                       max_val = d['value_max'],pft_ineq = d['pft_ineq'])\n",
    "                df.at[ensemble_member_row,target_col] = target_val\n",
    "                \n",
    "    print(\"Finished adding pft inequalities!\")\n",
    "            \n",
    "\n",
    "def check_na_and_zeros(df):\n",
    "    # Check for NA values in each column\n",
    "    na_count = df.isna().sum()\n",
    "    print(\"NA counts in each column:\")\n",
    "    print(na_count)\n",
    "\n",
    "    # Check for zeros in each column\n",
    "    zero_count = (df == 0).sum()\n",
    "    print(\"\\nZero counts in each column:\")\n",
    "    print(zero_count)\n",
    "    \n",
    "    \n",
    "\n",
    "def get_substring_before_underscore(s):\n",
    "    match = re.search(r'^(.*?)_(?=\\d)', s)\n",
    "    return match.group(1) if match else None\n",
    "\n",
    "def get_last_number(s):\n",
    "    matches = re.findall(r'\\d+', s)\n",
    "    return int(matches[-1]) if matches else None\n",
    "\n",
    "def assign_var_to_nc(file,var_name,value,index):\n",
    "    \n",
    "    '''\n",
    "    assigns a value to a netcdf for a particular pft and organ\n",
    "    \n",
    "    file = full path to netcdf\n",
    "    var_name = fates parameter name\n",
    "    value = parameter value to add to file\n",
    "    '''\n",
    "    \n",
    "    # open nc file\n",
    "    ncfile = nc.netcdf_file(file, 'a')\n",
    "    \n",
    "    # define param of interest\n",
    "    var = ncfile.variables[var_name]\n",
    "    \n",
    "    # get number of dimensions\n",
    "    ndim = len(var.dimensions)\n",
    "    \n",
    "    if var_name == \"fates_stoich_nitr\":\n",
    "        organ_index = 0\n",
    "        var[organ_index,index] = value\n",
    "    \n",
    "    elif \"fates_leafage_class\" in var.dimensions:\n",
    "        var[:,index] = value\n",
    "   \n",
    "    elif ndim == 0:\n",
    "        \n",
    "        var[...] = value\n",
    "    \n",
    "    else:\n",
    "        var[index] = value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70b2c54-8a51-41ed-ae79-e45cfda92486",
   "metadata": {},
   "source": [
    "## Create LHS without the ML model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb9f563-eb2f-4656-86d9-fd8418bc5e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create df of potential parameterizations\n",
    "lhs_df = generate_lhs_df(batch_size, seed, param_ranges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf81489e-1b8b-4316-9ad8-bb2387b95cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = add_inequality_cols(lhs_df,param_ranges_inequalities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b667a8a1-81c4-47b4-ad79-5483a8485660",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_parameter_values_for_inequalities(param_ranges_inequalities,df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f1f83b",
   "metadata": {},
   "source": [
    "## Generate promising ensemble members with the ML model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34c4561",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_promising = pd.DataFrame()\n",
    "\n",
    "# while len(df_promising) < target_ensemble_size:\n",
    "    \n",
    "#     if len(df_promising) == 0:\n",
    "#         seed = starting_seed\n",
    "#     else:\n",
    "#         seed = seed + 1\n",
    "    \n",
    "#     print(\"LHS seed:\",seed)\n",
    "    \n",
    "#     # Create df of potentail paramterizations\n",
    "#     lhs_df = generate_lhs_df(batch_size, seed, param_ranges)\n",
    "#     df = add_inequality_cols(lhs_df,param_ranges_inequalities)\n",
    "#     generate_parameter_values_for_inequalities(param_ranges_inequalities,df)\n",
    "\n",
    "#     # Check which ones will work\n",
    "#     X = transformation_pipeline.fit_transform(df[X_cols])\n",
    "#     probabilities = DNN_NFailed_PFTs.predict(X)\n",
    "#     y_pred = np.argmax(probabilities, axis=1)\n",
    "#     y_pred_0 = [i == 0 for i in y_pred]\n",
    "#     df_promising = pd.concat([df_promising,df[y_pred_0]])\n",
    "    \n",
    "#     print(len(df_promising), \"promising ensemble members so far...\")\n",
    "#     df_promising_reset = df_promising.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb116785",
   "metadata": {},
   "source": [
    "## Make subdirectories for parameter files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243da1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_file_subdirs = []\n",
    "for subdir_i in range(n_cases):\n",
    "    subdir_tag = str(subdir_i+1).rjust(2, '0')\n",
    "    param_file_subdirs.append(param_files_subdir_prefix +\"_\" + subdir_tag)\n",
    "print(param_file_subdirs)\n",
    "\n",
    "for i in range(len(param_file_subdirs)):\n",
    "    os.makedirs(os.path.join(path_to_ensemble_param_files_root,param_file_subdirs[i]),exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0bebc9",
   "metadata": {},
   "source": [
    "## Write promising parameterizations to netcdf files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53c21ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in FATES file with values that will be used for all non-varying parameters\n",
    "# This parameter file has many changes associated with it compared to the default FATES file\n",
    "# It also has new parameter added as part of the development required for this experiment.\n",
    "input_fname = os.path.join(path_to_base_param_files_root,base_param_file_name)\n",
    "\n",
    "print(input_fname)\n",
    "\n",
    "# For each sample\n",
    "row = -1\n",
    "\n",
    "# Loop over cases\n",
    "for subdir in param_file_subdirs:\n",
    "    \n",
    "    print(\"Working on\",subdir)\n",
    "    # Loop over instances in the case\n",
    "    for i in range(0,inst_per_case) :\n",
    "        \n",
    "        row = row + 1\n",
    "                \n",
    "        param_file_end = str(i+1).rjust(4, '0')\n",
    "\n",
    "        # final parameter file name\n",
    "        new_file_name = file_name_prefix + '_{0}.nc'.format(param_file_end)\n",
    "        fout = os.path.join(path_to_ensemble_param_files_root,subdir,new_file_name)\n",
    "\n",
    "        shutil.copyfile(input_fname, fout)                                                                                                                             \n",
    "      \n",
    "        # Loop through each parameter and apply either to the correct pft or globally\n",
    "        for col_name in sorted(list(df.columns)): # This was df_promising\n",
    "           \n",
    "            var_name = get_substring_before_underscore(col_name)\n",
    "            \n",
    "            index = max(0,get_last_number(col_name) - 1)\n",
    "            value = df.loc[row, col_name] # This was df_promising_reset\n",
    "            assign_var_to_nc(fout,var_name,value,index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452faa1a-64c9-468f-9fee-c31a2aa87286",
   "metadata": {},
   "source": [
    "## Check distribution of param values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1182fd-3625-4ae4-bde2-97f6b209f90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_param_dist(var = \"fates_allom_crown_depth_frac\",index = 0):\n",
    "\n",
    "    param_range = []\n",
    "    \n",
    "    #Loop through ensemble members\n",
    "    for subdir in param_file_subdirs:\n",
    "        \n",
    "        for inst in range(0,inst_per_case):\n",
    "    \n",
    "            # Get tag\n",
    "            param_file_end = str(inst+1).rjust(4, '0')\n",
    "    \n",
    "            # Get param file with inst tag\n",
    "            ref_nc_file = esm_tools.find_files_with_substring(directory=os.path.join(path_to_ensemble_param_files_root,subdir),\n",
    "                                                    substring=param_file_end)\n",
    "            \n",
    "            # Get full path of ref param file\n",
    "            ref_nc_file_full_path = os.path.join(path_to_ensemble_param_files_root,subdir,ref_nc_file[0])\n",
    "            \n",
    "            param_range.append(esm_tools.extract_variable_from_netcdf(ref_nc_file_full_path,var,index))\n",
    "            #print(param_range[inst])\n",
    "\n",
    "    plt.hist(param_range)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80190937-af6e-4620-a419-733d54989194",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_param_dist(var = \"fates_fire_alpha_SH\",index = 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env4_work",
   "language": "python",
   "name": "env4_work"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
